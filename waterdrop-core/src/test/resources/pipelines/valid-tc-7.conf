# batch processing
spark {

}

pipeline<p1> {
  input {
    file {
      path = "/tmp/waterdrop/stream1"
      format = "csv"
      table_name = "t1"
    }
  }

  filter {
    sql {
      sql = "select * from t1"
    }
  }
  output {
    stdout {}
  }
}

pipeline<p2> {
  input {
    file {
      path = "/tmp/waterdrop/stream2"
      format = "csv"
      table_name = "t2"
    }
  }

  filter {
    split {
      source_field = "_c1"
      delimiter = "-"
      fields = ["key", "value"]
    }
  }
  output {
    stdout {}
  }
}


